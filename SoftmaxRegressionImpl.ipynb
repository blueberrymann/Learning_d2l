{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from d2l import torch as d2l\n",
    "from IPython import display\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6.], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X  = torch.arange(1, 7, dtype=float)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.]], dtype=torch.float64),\n",
       " tensor([[5., 7., 9.]], dtype=torch.float64),\n",
       " tensor([[ 6.],\n",
       "         [15.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape(2, 3)\n",
    "X, X.sum(0, keepdim=True), X.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "  X_exp = torch.exp(X)\n",
    "  partition = X_exp.sum(1, keepdim=True)\n",
    "  return X_exp / partition # 应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0746, 0.6207, 0.1210, 0.1177, 0.0660],\n",
       "        [0.1424, 0.2357, 0.2937, 0.1350, 0.1931]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.normal(0, 1, (2, 5))\n",
    "Y_prob = softmax(Y)\n",
    "Y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prob.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "  return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.2, 0.3, 0.5]])\n",
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵损失\n",
    "\n",
    "- 交叉熵通常用来衡量两个概率的区别\n",
    "$$\n",
    "H(\\mathbf{p,q}) = \\sum_i-p_ilog(q_i)\n",
    "$$\n",
    "\n",
    "- 将交叉熵作为损失，由于真实$y$中只有一个变量为1，所以我们的损失结构也就是对正确类的概率估计\n",
    "$$\n",
    "l(\\mathbf{y},\\hat{\\mathbf{y}}) = -\\sum_iy_i\\log\\hat{y_i} = -\\log\\hat{y_y}\n",
    "$$\n",
    "\n",
    "- 其梯度是真实概率和预测概率的区别\n",
    "$$\n",
    "\\frac{\\partial l(\\mathbf{y},\\hat{\\mathbf{y}})}{\\partial o_i} = \\text{softmax}(o_i) - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "  # 其中len的值就是样本批次\n",
    "  # y中存储的就是对应的真实类别（序号）\n",
    "  # y_hat中存储的多个行向量\n",
    "  # 每个行向量对应一个批次，都是一个概率分布\n",
    "  # 行向量中的元素对应一个概率\n",
    "  return -torch.log(y_hat[range(len(y_hat)), y])\n",
    "\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_hat, y):\n",
    "  if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "    # 将每个行向量中最大值（即预测的类别）的下标返回\n",
    "    y_hat = y_hat.argmax(axis=1)\n",
    "  cmp = y_hat.type(y.dtype) == y\n",
    "  return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "\n",
    "accuracy(y_hat, y) / len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "  # 在n个变量上累加\n",
    "  def __init__(self, n):\n",
    "    # 初始化n个累加器，每个累加器初始值为0\n",
    "    self.data = [0.0] * n\n",
    "  \n",
    "  def add(self, *args):\n",
    "    # 将传入的参数依次累加到对应的累加器上\n",
    "    self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "  \n",
    "  def reset(self):\n",
    "    # 重置所有累加器为0\n",
    "    self.data = [0.0] * len(self.data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    # 获取第idx个累加器的值\n",
    "    return self.data[idx]\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "  # 如果net是torch.nn.Module的实例，则设置为评估模式\n",
    "  if isinstance(net, torch.nn.Module):\n",
    "    net.eval()\n",
    "  # 创建一个Accumulator实例，用于累加正确预测的数量和总样本数量\n",
    "  metric = Accumulator(2)\n",
    "  for X, y in data_iter:\n",
    "    # 累加正确预测的数量和总样本数量\n",
    "    metric.add(accuracy(net(X), y), y.numel())\n",
    "  # 返回正确预测的比例\n",
    "  return metric[0] / metric[1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
